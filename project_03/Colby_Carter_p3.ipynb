{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Poisonous mushrooms\n",
    "\n",
    "# Author: Colby Carter\n",
    "\n",
    "## Due: December 4, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll investigate properties of mushrooms. This classic dataset contains over 8000 observations, where each mushroom is described by a variety of features like color, odor, etc., and the target variable is an indicator for whether the mushroom is poisonous. Since all the observations are categorical, I've binarized the feature space. Look at the feature_names below to see all 126 binary names.\n",
    "\n",
    "You'll start by running PCA to reduce the dimensionality from 126 down to 2 so that you can easily visualize the data. In general, PCA is very useful for visualization (though sklearn.manifold.tsne is known to produce better visualizations). Recall that PCA is a linear transformation. The 1st projected dimension is the linear combination of all 126 original features that captures as much of the variance in the data as possible. The 2nd projected dimension is the linear combination of all 126 original features that captures as much of the remaining variance as possible. The idea of dense low dimensional representations is crucial to machine learning!\n",
    "\n",
    "Once you've projected the data to 2 dimensions, you'll experiment with clustering using KMeans and density estimation with Gaussian Mixture Models. Finally, you'll train a classifier by fitting a GMM for the positive class and a GMM for the negative class, and perform inference by comparing the probabilities output by each model.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "#from sklearn.mixture import GMM\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded feature names: 126\n",
      "<class '_io.TextIOWrapper'>\n"
     ]
    }
   ],
   "source": [
    "feature_names = []\n",
    "with open('mushroom.map') as fmap:\n",
    "    for line in fmap:\n",
    "        [index, name, junk] = line.split()\n",
    "        feature_names.append(name)\n",
    "\n",
    "print('Loaded feature names:', len(feature_names))\n",
    "print(type(fmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data. The data is sparse in the input file, but there aren't too many features, so we'll use a dense representation, which is supported by all sklearn objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 126) (1124, 126)\n"
     ]
    }
   ],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "with open('mushroom.data') as fdata:\n",
    "    for line in fdata:\n",
    "        items = line.split()\n",
    "        Y.append(int(items.pop(0)))\n",
    "        x = np.zeros(len(feature_names))\n",
    "        for item in items:\n",
    "            feature = int(item.split(':')[0])\n",
    "            x[feature] = 1\n",
    "        X.append(x)\n",
    "\n",
    "# Convert these lists to numpy arrays.\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Split into train and test data.\n",
    "train_data, train_labels = X[:7000], Y[:7000]\n",
    "test_data, test_labels = X[7000:], Y[7000:]\n",
    "\n",
    "# Check that the shapes look right.\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Run Principal Components Analysis on the data. Show what fraction of the total variance in the training data is explained by the first k principal components, for k in [1, 50]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 = 0.000\n",
      "Feature 2 = 0.164\n",
      "Feature 3 = 0.297\n",
      "Feature 4 = 0.399\n",
      "Feature 5 = 0.470\n",
      "Feature 6 = 0.508\n",
      "Feature 7 = 0.545\n",
      "Feature 8 = 0.576\n",
      "Feature 9 = 0.604\n",
      "Feature 10 = 0.630\n",
      "Feature 11 = 0.652\n",
      "Feature 12 = 0.673\n",
      "Feature 13 = 0.691\n",
      "Feature 14 = 0.709\n",
      "Feature 15 = 0.726\n",
      "Feature 16 = 0.741\n",
      "Feature 17 = 0.756\n",
      "Feature 18 = 0.770\n",
      "Feature 19 = 0.784\n",
      "Feature 20 = 0.798\n",
      "Feature 21 = 0.809\n",
      "Feature 22 = 0.820\n",
      "Feature 23 = 0.830\n",
      "Feature 24 = 0.841\n",
      "Feature 25 = 0.851\n",
      "Feature 26 = 0.860\n",
      "Feature 27 = 0.868\n",
      "Feature 28 = 0.876\n",
      "Feature 29 = 0.884\n",
      "Feature 30 = 0.892\n",
      "Feature 31 = 0.899\n",
      "Feature 32 = 0.905\n",
      "Feature 33 = 0.911\n",
      "Feature 34 = 0.917\n",
      "Feature 35 = 0.922\n",
      "Feature 36 = 0.927\n",
      "Feature 37 = 0.932\n",
      "Feature 38 = 0.937\n",
      "Feature 39 = 0.942\n",
      "Feature 40 = 0.947\n",
      "Feature 41 = 0.951\n",
      "Feature 42 = 0.955\n",
      "Feature 43 = 0.959\n",
      "Feature 44 = 0.963\n",
      "Feature 45 = 0.966\n",
      "Feature 46 = 0.970\n",
      "Feature 47 = 0.973\n",
      "Feature 48 = 0.975\n",
      "Feature 49 = 0.978\n",
      "Feature 50 = 0.980\n",
      "\n",
      "With 98% of the training set's variance explained by the first 50 features, there is little need for the k>50 features.\n"
     ]
    }
   ],
   "source": [
    "def P1():\n",
    "### STUDENT START ###\n",
    "    pca_1 = PCA()\n",
    "    pca_1.fit(train_data)\n",
    "    for k in range(0,50):\n",
    "        print(\"Feature\",k+1,\"=\",\"{:.3f}\".format(np.sum(pca_1.explained_variance_ratio_[0:k])))\n",
    "    print(\"\\nWith 98% of the training set's variance explained by the first 50 features, there is little need for the k>50 features.\")\n",
    "### STUDENT END ###\n",
    "\n",
    "P1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) PCA can be very useful for visualizing data. Project the training data down to 2 dimensions and plot it. Show the positive (poisonous) cases in blue and the negative (non-poisonous) in red. Here's a reference for plotting: http://matplotlib.org/users/pyplot_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False  True False False False  True False False]\n",
      "train pos= (3198, 2)\n",
      "train neg= (3802, 2)\n",
      "(3198,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndgBoDMEOAI0h2AGgMQQ7ADSGYAeAxvw/Fy61B/+/ZJ8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21a951a7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def P2():\n",
    "### STUDENT START ###\n",
    "    pca_2 = PCA(n_components=2)\n",
    "    train_data2 = pca_2.fit_transform(train_data)\n",
    "    \n",
    "    label_pos, label_neg = (train_labels[:] == 1), (train_labels[:] == 0)\n",
    "    print(label_pos[0:10])\n",
    "    train_pos, train_neg = train_data2[label_pos], train_data2[label_neg]\n",
    "    print(\"train pos=\",train_pos.shape)\n",
    "    print(\"train neg=\",train_neg.shape)\n",
    "    print(train_pos[:,1].shape)\n",
    "    plt.plot(train_pos[:,0], train_pos[:,1], 'bs', train_neg[:,0], train_neg[:,1], 'rs',)\n",
    "    \n",
    "    #poison = train_data2[train_data[:]==1,:]\n",
    "    \n",
    "### STUDENT END ###\n",
    "\n",
    "P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Run KMeans with [1,16] clusters over the 2d projected data. Mark each centroid cluster and plot a circle that goes through the most distant point assigned to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def P3():\n",
    "### STUDENT START ###\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Fit a Gaussian Mixture Model for the positive examples in your 2d projected data. Plot the estimated density contours as shown here: http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html#example-mixture-plot-gmm-pdf-py. Vary the number of mixture components from 1-4 and the covariance matrix type ('spherical', 'diag', 'tied', 'full')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def P4():\n",
    "### STUDENT START ###\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Fit two 4-component full covariance GMMs, one for the positive examples and one for the negative examples in your 2d projected data. Predict the test examples by choosing the label for which the model gives a larger probability (use GaussianMixture.score_samples). What is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def P5():\n",
    "### STUDENT START ###\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Ideally, we'd like a model that gives the best accuracy with the fewest parameters. Run a series of experiments to find the model that gives the best accuracy with no more than 50 parameters. For example, with 3 PCA components and 2-component diagonal covariance GMMs, you'd have:\n",
    "\n",
    "( (3 mean vector + 3 covariance matrix) x 2 components ) x 2 classes = 24 parameters\n",
    "\n",
    "You should vary the number of PCA components, the number of GMM components, and the covariance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def P6():\n",
    "### STUDENT START ###\n",
    "\n",
    "### STUDENT END ###\n",
    "\n",
    "#P6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
